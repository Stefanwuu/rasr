# I6 TF image using Ubuntu 22.04 and Cuda 12.3.2 from official nvidia images
# For release notes see https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-23-12.html
Bootstrap: docker
From: nvcr.io/nvidia/pytorch:23.12-py3
Stage: build

%post
    apt update -y

    # all the fundamental basics, zsh is need because calling the cache manager might launch the user shell
    DEBIAN_FRONTEND=noninteractive apt install -y wget git unzip gzip libssl-dev lsb-release zsh \
        bison libxml2-dev libopenblas-dev libsndfile1-dev libcrypto++-dev libcppunit-dev \
        parallel xmlstarlet python3-lxml htop strace gdb sox python3-pip cmake

    # download the cache manager and place in /usr/local
    cd /usr/local
    git clone https://github.com/rwth-i6/cache-manager.git
    cd bin
    ln -s ../cache-manager/cf cf

    cd /usr/local
    wget -q https://github.com/microsoft/onnxruntime/releases/download/v1.16.3/onnxruntime-linux-x64-gpu-1.16.3.tgz
    tar xzvf onnxruntime-linux-x64-*.tgz
    mkdir -p /usr/local/{include,lib}
    mv onnxruntime-linux-x64-*/include/* /usr/local/include/
    mv onnxruntime-linux-x64-*/lib/* /usr/local/lib/
    rm -r onnxruntime-linux-x64-*
    ldconfig

